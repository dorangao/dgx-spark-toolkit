# RayCluster for Distributed vLLM with Pipeline Parallelism
#
# Uses KubeRay operator to manage Ray cluster lifecycle.
# Deploys Nemotron-3 Nano 30B across 2 DGX Spark nodes using pipeline parallelism.
#
# Architecture:
#   - Head node (spark-2959): Ray head + 1 GPU worker
#   - Worker node (spark-ba63): Ray worker + 1 GPU
#   - Pipeline Parallelism: 2 (splits model layers across nodes)
#   - Communication: 200GbE fabric network (10.10.10.x)
#
# After cluster is ready, run:
#   kubectl exec -n llm-inference <head-pod> -- vllm serve ...
#
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: vllm-cluster
  namespace: llm-inference
  labels:
    app.kubernetes.io/name: vllm-distributed
    app.kubernetes.io/component: ray-cluster
spec:
  rayVersion: '2.40.0'
  enableInTreeAutoscaling: false
  
  # Head node configuration
  headGroupSpec:
    rayStartParams:
      dashboard-host: '0.0.0.0'
      num-gpus: '1'
      # Use fabric network for Ray communication
      node-ip-address: '10.10.10.1'
    
    template:
      metadata:
        labels:
          app: vllm-ray-head
          ray-cluster: vllm-cluster
          ray-node-type: head
      spec:
        runtimeClassName: nvidia
        
        # Pin head to spark-2959
        nodeSelector:
          kubernetes.io/hostname: spark-2959
        
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
        
        # Use host network for fabric access
        hostNetwork: true
        dnsPolicy: ClusterFirstWithHostNet
        
        containers:
          - name: ray-head
            image: avarok/vllm-dgx-spark:v11
            imagePullPolicy: IfNotPresent
            
            ports:
              - containerPort: 6379
                name: gcs
              - containerPort: 8265
                name: dashboard
              - containerPort: 10001
                name: client
            
            env:
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token-secret
                    key: HF_TOKEN
              - name: HF_HOME
                value: /models/.cache
              - name: TRANSFORMERS_CACHE
                value: /models/.cache
              - name: HF_HUB_CACHE
                value: /models/.cache/hub
              - name: VLLM_HOST_IP
                value: "10.10.10.1"
              # Ray address for vLLM to connect to
              - name: RAY_ADDRESS
                value: "10.10.10.1:6379"
              # Disable CUDA graphs for Blackwell stability
              - name: VLLM_USE_CUDA_GRAPH
                value: "0"
            
            resources:
              requests:
                nvidia.com/gpu: 1
                cpu: "8"
                memory: "64Gi"
              limits:
                nvidia.com/gpu: 1
                cpu: "16"
                memory: "96Gi"
            
            volumeMounts:
              - name: model-cache
                mountPath: /models
              - name: dshm
                mountPath: /dev/shm
        
        volumes:
          - name: model-cache
            persistentVolumeClaim:
              claimName: vllm-models-pvc
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: "16Gi"
  
  # Worker node configuration
  workerGroupSpecs:
    - groupName: gpu-workers
      replicas: 1
      minReplicas: 1
      maxReplicas: 1
      
      rayStartParams:
        num-gpus: '1'
        # Use fabric network for Ray communication
        node-ip-address: '10.10.10.2'
      
      template:
        metadata:
          labels:
            app: vllm-ray-worker
            ray-cluster: vllm-cluster
            ray-node-type: worker
        spec:
          runtimeClassName: nvidia
          
          # Pin worker to spark-ba63
          nodeSelector:
            kubernetes.io/hostname: spark-ba63
          
          tolerations:
            - key: nvidia.com/gpu
              operator: Exists
              effect: NoSchedule
          
          # Use host network for fabric access
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          
          containers:
            - name: ray-worker
              image: avarok/vllm-dgx-spark:v11
              imagePullPolicy: IfNotPresent
              
              env:
                - name: HF_TOKEN
                  valueFrom:
                    secretKeyRef:
                      name: hf-token-secret
                      key: HF_TOKEN
                - name: HF_HOME
                  value: /models/.cache
                - name: TRANSFORMERS_CACHE
                  value: /models/.cache
                - name: HF_HUB_CACHE
                  value: /models/.cache/hub
                - name: VLLM_HOST_IP
                  value: "10.10.10.2"
                - name: RAY_ADDRESS
                  value: "10.10.10.1:6379"
                - name: VLLM_USE_CUDA_GRAPH
                  value: "0"
              
              resources:
                requests:
                  nvidia.com/gpu: 1
                  cpu: "8"
                  memory: "64Gi"
                limits:
                  nvidia.com/gpu: 1
                  cpu: "16"
                  memory: "96Gi"
              
              volumeMounts:
                - name: model-cache
                  mountPath: /models
                - name: dshm
                  mountPath: /dev/shm
          
          volumes:
            # Worker needs local storage (PVC is RWO)
            - name: model-cache
              emptyDir:
                sizeLimit: "100Gi"
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: "16Gi"
---
# Service to expose vLLM API via fabric network
# Note: vLLM runs on port 8080 via hostNetwork, accessible at 10.10.10.1:8080
apiVersion: v1
kind: Service
metadata:
  name: vllm-distributed-service
  namespace: llm-inference
  labels:
    app.kubernetes.io/name: vllm-distributed
spec:
  selector:
    ray-node-type: head
    ray-cluster: vllm-cluster
  ports:
    - name: ray-dashboard
      port: 8265
      targetPort: 8265
  type: ClusterIP
